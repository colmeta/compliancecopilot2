"""
COMPLETE FUNDING ENGINE - Presidential Quality
Full workflow: Generate â†’ Convert â†’ Package â†’ Email
"""

from flask import Blueprint, jsonify, request
import uuid
import os
import tempfile
from datetime import datetime
import logging
from app.funding.document_generator import get_document_generator
from app.funding.document_converter import get_converter
from app.funding.package_manager import get_package_manager
from app.funding.document_analyzer import get_document_analyzer
from app.funding.gap_analyzer import get_gap_analyzer
from app.funding.document_refiner import get_document_refiner
from app.email_service import EmailService

logger = logging.getLogger(__name__)

real_funding_v2 = Blueprint('real_funding_v2', __name__)


@real_funding_v2.route('/v2/funding/generate', methods=['GET', 'POST'])
def generate_complete_package():
    """
    COMPLETE PRESIDENTIAL-GRADE WORKFLOW
    
    GET: Returns endpoint information
    POST: Generates funding package
    
    Steps:
    1. Generate 20 Markdown documents with AI
    2. Convert to PDF + Word + PowerPoint
    3. Package into ZIP file
    4. Upload to cloud storage (optional)
    5. Send via email with download link
    
    POST /v2/funding/generate
    Body: {
        "email": "user@company.com",
        "discovery_answers": {
            "company_name": "...",
            "industry": "...",
            "problem": "...",
            "solution": "...",
            ...
        },
        "config": {
            "fundingLevel": "seed",
            "selectedDocuments": ["vision", "pitch_deck", ...],
            "formats": ["pdf", "word", "pptx"],
            "delivery": "email"  # or "download" or "both"
        }
    }
    """
    if request.method == 'GET':
        # Return endpoint information for GET requests
        return jsonify({
            'endpoint': '/v2/funding/generate',
            'method': 'POST',
            'description': 'Generate complete presidential-grade funding package',
            'required_fields': {
                'email': 'string (required)',
                'discovery_answers': 'object (required)',
                'config': 'object (required)'
            },
            'example_request': {
                'email': 'user@company.com',
                'discovery_answers': {
                    'company_name': 'Your Company',
                    'industry': 'Technology',
                    'problem': 'Problem statement',
                    'solution': 'Your solution'
                },
                'config': {
                    'fundingLevel': 'seed',
                    'selectedDocuments': ['vision', 'pitch_deck'],
                    'formats': ['pdf', 'word', 'pptx'],
                    'delivery': 'email'
                }
            },
            'note': 'Use POST method with JSON body to generate documents'
        }), 200
    
    # POST method continues below
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        # Extract data
        email = data.get('email', '').strip()
        discovery_answers = data.get('discovery_answers', {})
        documents = data.get('documents', [])  # Optional: uploaded documents
        config = data.get('config', {})
        
        # Validate
        if not email or '@' not in email:
            return jsonify({'error': 'Valid email address required'}), 400
        
        selected_documents = config.get('selectedDocuments', [])
        if not selected_documents:
            return jsonify({'error': 'Select at least one document'}), 400
        
        funding_level = config.get('fundingLevel', 'seed')
        output_formats = config.get('formats', ['pdf', 'word'])
        delivery_method = config.get('delivery', 'email')
        
        company_name = discovery_answers.get('company_name', 'Company')
        task_id = str(uuid.uuid4())
        
        # Check if using document-first workflow
        extracted_info = None
        if documents:
            # Document-first workflow: analyze documents first
            analyzer = get_document_analyzer()
            analysis_result = analyzer.analyze_documents(documents)
            
            if analysis_result.get('success'):
                extracted_info = analysis_result.get('extracted_info', {})
                logger.info(f"[{task_id}] Extracted info from {len(documents)} document(s)")
            else:
                logger.warning(f"[{task_id}] Document analysis failed: {analysis_result.get('error')}")
                # Continue with question-based workflow if analysis fails
        
        if not discovery_answers and not extracted_info:
            return jsonify({'error': 'Either discovery answers or documents required'}), 400
        
        logger.info(f"[{task_id}] Starting COMPLETE package generation for {company_name}")
        
        # ========================================
        # STEP 1: AI GENERATION (Markdown)
        # ========================================
        logger.info(f"[{task_id}] Step 1/5: Generating {len(selected_documents)} documents with AI...")
        
        generator = get_document_generator()
        
        if not generator.enabled:
            return jsonify({
                'success': False,
                'error': 'AI not configured',
                'message': 'GOOGLE_API_KEY not set - please configure',
                'task_id': task_id
            }), 503
        
        generation_result = generator.generate_package(
            discovery_answers=discovery_answers,
            funding_level=funding_level,
            selected_documents=selected_documents,
            extracted_info=extracted_info  # Pass extracted info if available
        )
        
        if not generation_result['success']:
            return jsonify({
                'success': False,
                'error': generation_result.get('error'),
                'message': 'AI generation failed',
                'task_id': task_id
            }), 500
        
        logger.info(f"[{task_id}] âœ… Generated {generation_result['completed']} documents ({generation_result['total_pages']} pages)")
        
        # ========================================
        # STEP 2: DOCUMENT CONVERSION
        # ========================================
        logger.info(f"[{task_id}] Step 2/5: Converting to {output_formats}...")
        
        converter = get_converter()
        temp_dir = tempfile.mkdtemp()
        all_files = {}
        
        for doc_info in generation_result['documents']:
            if not doc_info['success']:
                continue
            
            doc_id = doc_info['id']
            markdown_content = doc_info['content']
            
            # Determine formats (PPT only for pitch deck)
            doc_formats = output_formats.copy()
            if doc_id == 'pitch_deck' and 'pptx' not in doc_formats:
                doc_formats.append('pptx')
            elif doc_id != 'pitch_deck' and 'pptx' in doc_formats:
                doc_formats.remove('pptx')
            
            # Convert with metadata
            metadata = {
                'company_name': company_name,
                'document_type': doc_info['name'],
                'funding_level': funding_level,
                'pages': doc_info.get('pages', 0)
            }
            
            try:
                converted_files = converter.convert_document(
                    markdown_content=markdown_content,
                    document_id=doc_id,
                    output_dir=temp_dir,
                    metadata=metadata,
                    formats=doc_formats
                )
                all_files.update(converted_files)
                logger.info(f"[{task_id}] âœ… Converted {doc_id} to {len(converted_files)} format(s)")
            except Exception as e:
                logger.error(f"[{task_id}] âŒ Failed to convert {doc_id}: {e}")
        
        logger.info(f"[{task_id}] âœ… Converted {len(all_files)} files total")
        
        # ========================================
        # STEP 3: ZIP PACKAGING
        # ========================================
        logger.info(f"[{task_id}] Step 3/5: Creating ZIP package...")
        
        package_manager = get_package_manager()
        package_result = package_manager.package_and_upload(
            file_paths=all_files,
            temp_dir=temp_dir,
            company_name=company_name
        )
        
        if not package_result['success']:
            return jsonify({
                'success': False,
                'error': package_result.get('error'),
                'message': 'Packaging failed',
                'task_id': task_id
            }), 500
        
        logger.info(f"[{task_id}] âœ… Package created: {package_result['zip_size_mb']} MB")
        
        # ========================================
        # STEP 4: CLOUD UPLOAD (Optional)
        # ========================================
        logger.info(f"[{task_id}] Step 4/5: Cloud upload...")
        download_url = package_result.get('download_url')
        
        if download_url:
            logger.info(f"[{task_id}] âœ… Uploaded to cloud storage")
        else:
            logger.info(f"[{task_id}] âš ï¸  S3 not configured - using local storage")
        
        # ========================================
        # STEP 5: EMAIL DELIVERY
        # ========================================
        email_sent = False
        
        if delivery_method in ['email', 'both']:
            logger.info(f"[{task_id}] Step 5/5: Sending email to {email}...")
            
            try:
                email_service = EmailService()
                
                if email_service.is_configured():
                    email_sent = email_service.send_funding_package_email(
                        to_email=email,
                        company_name=company_name,
                        documents=[doc for doc in generation_result['documents'] if doc['success']],
                        zip_path=package_result['zip_path'],
                        download_url=download_url,
                        package_size_mb=package_result['zip_size_mb']
                    )
                    
                    if email_sent:
                        logger.info(f"[{task_id}] âœ… Email sent successfully")
                    else:
                        logger.warning(f"[{task_id}] âš ï¸  Email sending failed")
                else:
                    logger.warning(f"[{task_id}] âš ï¸  Email not configured - skipping")
            except Exception as e:
                logger.error(f"[{task_id}] âŒ Email error: {e}")
        
        # ========================================
        # RETURN SUCCESS
        # ========================================
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'Presidential-grade funding package generated successfully! ðŸŽ‰',
            
            'generation': {
                'documents_generated': generation_result['completed'],
                'documents_failed': generation_result['failed'],
                'total_pages': generation_result['total_pages'],
                'generation_time': generation_result['generation_time']
            },
            
            'conversion': {
                'files_created': len(all_files),
                'formats': list(set([os.path.splitext(f)[1][1:] for f in all_files.values()]))
            },
            
            'package': {
                'filename': package_result['zip_filename'],
                'size_mb': package_result['zip_size_mb'],
                'storage': package_result['storage'],
                'download_url': download_url,
                'expires_in_days': 7 if download_url else None
            },
            
            'delivery': {
                'method': delivery_method,
                'email_sent': email_sent,
                'email': email if email_sent else None
            },
            
            'documents': [
                {
                    'id': doc['id'],
                    'name': doc['name'],
                    'category': doc['category'],
                    'pages': doc.get('pages', 0),
                    'success': doc['success']
                }
                for doc in generation_result['documents']
            ],
            
            'quality': 'Presidential / Fortune 50 / Y-Combinator',
            'timestamp': datetime.utcnow().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Complete workflow failed: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'Document generation workflow failed',
            'task_id': task_id if 'task_id' in locals() else None
        }), 500


@real_funding_v2.route('/v2/funding/analyze-documents', methods=['POST'])
def analyze_documents():
    """
    Analyze uploaded documents and extract information.
    
    POST /v2/funding/analyze-documents
    Body: {
        "documents": [
            {
                "filename": "pitch_deck.pdf",
                "content_base64": "...",
                "content_type": "application/pdf"
            }
        ],
        "funding_level": "seed"  # optional
    }
    
    Returns:
        {
            "success": true,
            "extracted_info": {...},
            "confidence_scores": {...},
            "gaps": [...],
            "questions": [...]
        }
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        documents = data.get('documents', [])
        funding_level = data.get('funding_level', 'seed')
        
        if not documents:
            return jsonify({'error': 'No documents provided'}), 400
        
        logger.info(f"Analyzing {len(documents)} document(s) for {funding_level} funding")
        
        # Step 1: Analyze documents
        analyzer = get_document_analyzer()
        analysis_result = analyzer.analyze_documents(documents)
        
        if not analysis_result.get('success'):
            return jsonify({
                'success': False,
                'error': analysis_result.get('error', 'Document analysis failed'),
                'message': 'Failed to extract information from documents'
            }), 500
        
        extracted_info = analysis_result.get('extracted_info', {})
        confidence_scores = analysis_result.get('confidence_scores', {})
        document_types = analysis_result.get('document_types', [])
        
        # Step 2: Identify gaps
        gap_analyzer = get_gap_analyzer()
        gaps_result = gap_analyzer.identify_gaps(
            extracted_info=extracted_info,
            funding_level=funding_level
        )
        
        # Format questions from gaps
        questions = [
            {
                'id': f"gap_{i+1}",
                'field': gap['field'],
                'question': gap['question'],
                'priority': gap['priority'],
                'why_important': gap.get('why_important', '')
            }
            for i, gap in enumerate(gaps_result.get('gaps', []))
        ]
        
        return jsonify({
            'success': True,
            'extracted_info': extracted_info,
            'confidence_scores': confidence_scores,
            'document_types': document_types,
            'gaps': gaps_result.get('gaps', []),
            'questions': questions,
            'completeness_score': gaps_result.get('completeness_score', 0.0),
            'processing_summary': analysis_result.get('processing_summary', {}),
            'message': f"Extracted information from {len(documents)} document(s). {len(questions)} question(s) needed to complete."
        }), 200
        
    except Exception as e:
        logger.error(f"Document analysis failed: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'Document analysis failed'
        }), 500


@real_funding_v2.route('/v2/funding/generate-from-documents', methods=['POST'])
def generate_from_documents():
    """
    Complete document-first workflow: Analyze â†’ Ask Questions â†’ Generate/Refine
    
    POST /v2/funding/generate-from-documents
    Body: {
        "email": "user@company.com",
        "documents": [...],  # Uploaded documents
        "discovery_answers": {...},  # Answers to gap questions (optional)
        "config": {
            "fundingLevel": "seed",
            "selectedDocuments": ["vision", "pitch_deck", ...],
            "formats": ["pdf", "word", "pptx"],
            "delivery": "email",
            "refine_existing": true  # Refine uploaded docs vs generate new
        }
    }
    """
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        email = data.get('email', '').strip()
        documents = data.get('documents', [])
        discovery_answers = data.get('discovery_answers', {})
        config = data.get('config', {})
        
        # Validate
        if not email or '@' not in email:
            return jsonify({'error': 'Valid email address required'}), 400
        
        if not documents:
            return jsonify({'error': 'Documents required for document-first workflow'}), 400
        
        selected_documents = config.get('selectedDocuments', [])
        if not selected_documents:
            return jsonify({'error': 'Select at least one document to generate'}), 400
        
        funding_level = config.get('fundingLevel', 'seed')
        output_formats = config.get('formats', ['pdf', 'word'])
        delivery_method = config.get('delivery', 'email')
        refine_existing = config.get('refine_existing', False)
        
        task_id = str(uuid.uuid4())
        logger.info(f"[{task_id}] Starting document-first workflow for {email}")
        
        # Step 1: Analyze documents
        logger.info(f"[{task_id}] Step 1/6: Analyzing {len(documents)} document(s)...")
        analyzer = get_document_analyzer()
        analysis_result = analyzer.analyze_documents(documents)
        
        if not analysis_result.get('success'):
            return jsonify({
                'success': False,
                'error': analysis_result.get('error', 'Document analysis failed'),
                'task_id': task_id
            }), 500
        
        extracted_info = analysis_result.get('extracted_info', {})
        document_types = analysis_result.get('document_types', [])
        
        # Step 2: Identify gaps
        logger.info(f"[{task_id}] Step 2/6: Identifying information gaps...")
        gap_analyzer = get_gap_analyzer()
        gaps_result = gap_analyzer.identify_gaps(
            extracted_info=extracted_info,
            funding_level=funding_level
        )
        
        # If there are gaps and no answers provided, return gaps for user to answer
        if gaps_result.get('missing_count', 0) > 0 and not discovery_answers:
            questions = [
                {
                    'id': f"gap_{i+1}",
                    'field': gap['field'],
                    'question': gap['question'],
                    'priority': gap['priority'],
                    'why_important': gap.get('why_important', '')
                }
                for i, gap in enumerate(gaps_result.get('gaps', []))
            ]
            
            return jsonify({
                'success': True,
                'status': 'questions_needed',
                'task_id': task_id,
                'extracted_info': extracted_info,
                'completeness_score': gaps_result.get('completeness_score', 0.0),
                'questions': questions,
                'message': f"Extracted information from documents. Please answer {len(questions)} question(s) to complete.",
                'next_step': 'Submit answers via discovery_answers field and call this endpoint again'
            }), 200
        
        # Step 3: Merge information
        logger.info(f"[{task_id}] Step 3/6: Merging extracted info with user answers...")
        # This is handled in document_generator.generate_package()
        
        # Step 4: Generate/Refine documents
        logger.info(f"[{task_id}] Step 4/6: Generating/refining documents...")
        generator = get_document_generator()
        
        if not generator.enabled:
            return jsonify({
                'success': False,
                'error': 'AI not configured',
                'task_id': task_id
            }), 503
        
        # Determine which documents to refine vs generate new
        documents_to_refine = []
        documents_to_generate = []
        
        if refine_existing:
            # Refine existing documents that match uploaded types
            for doc_type in document_types:
                if doc_type in selected_documents:
                    documents_to_refine.append(doc_type)
                else:
                    documents_to_generate.append(doc_type)
            
            # Generate new documents not in uploaded types
            for doc_id in selected_documents:
                if doc_id not in document_types:
                    documents_to_generate.append(doc_id)
        else:
            # Generate all documents (new)
            documents_to_generate = selected_documents
        
        # Generate new documents
        generation_result = generator.generate_package(
            discovery_answers=discovery_answers,
            funding_level=funding_level,
            selected_documents=documents_to_generate,
            extracted_info=extracted_info
        )
        
        if not generation_result['success']:
            return jsonify({
                'success': False,
                'error': generation_result.get('error'),
                'task_id': task_id
            }), 500
        
        # Refine existing documents (if requested)
        refiner = get_document_refiner()
        refined_documents = []
        
        if refine_existing and documents_to_refine:
            logger.info(f"[{task_id}] Refining {len(documents_to_refine)} existing document(s)...")
            # Note: In a full implementation, you'd extract original document text
            # For now, we'll generate them (refinement can be added later)
            for doc_type in documents_to_refine:
                refine_result = refiner.generate_new_document(
                    doc_type=doc_type,
                    extracted_info=extracted_info,
                    new_answers=discovery_answers,
                    funding_level=funding_level,
                    existing_docs=document_types
                )
                if refine_result['success']:
                    refined_documents.append({
                        'id': doc_type,
                        'name': doc_type.replace('_', ' ').title(),
                        'content': refine_result['content'],
                        'success': True,
                        'refined': True
                    })
        
        # Combine generated and refined documents
        all_documents = generation_result['documents'] + refined_documents
        company_name = extracted_info.get('company_name') or discovery_answers.get('company_name', 'Company')
        
        # Step 5: Convert documents (same as existing workflow)
        logger.info(f"[{task_id}] Step 5/6: Converting documents...")
        converter = get_converter()
        temp_dir = tempfile.mkdtemp()
        all_files = {}
        
        for doc_info in all_documents:
            if not doc_info.get('success'):
                continue
            
            doc_id = doc_info.get('id')
            markdown_content = doc_info.get('content')
            
            if not markdown_content:
                continue
            
            doc_formats = output_formats.copy()
            if doc_id == 'pitch_deck' and 'pptx' not in doc_formats:
                doc_formats.append('pptx')
            elif doc_id != 'pitch_deck' and 'pptx' in doc_formats:
                doc_formats.remove('pptx')
            
            metadata = {
                'company_name': company_name,
                'document_type': doc_info.get('name', doc_id),
                'funding_level': funding_level
            }
            
            try:
                converted_files = converter.convert_document(
                    markdown_content=markdown_content,
                    document_id=doc_id,
                    output_dir=temp_dir,
                    metadata=metadata,
                    formats=doc_formats
                )
                all_files.update(converted_files)
            except Exception as e:
                logger.error(f"[{task_id}] Failed to convert {doc_id}: {e}")
        
        # Step 6: Package and deliver (same as existing workflow)
        logger.info(f"[{task_id}] Step 6/6: Packaging and delivering...")
        package_manager = get_package_manager()
        package_result = package_manager.package_and_upload(
            file_paths=all_files,
            temp_dir=temp_dir,
            company_name=company_name
        )
        
        if not package_result['success']:
            return jsonify({
                'success': False,
                'error': package_result.get('error'),
                'task_id': task_id
            }), 500
        
        download_url = package_result.get('download_url')
        email_sent = False
        
        if delivery_method in ['email', 'both']:
            try:
                email_service = EmailService()
                if email_service.is_configured():
                    email_sent = email_service.send_funding_package_email(
                        to_email=email,
                        company_name=company_name,
                        documents=all_documents,
                        zip_path=package_result['zip_path'],
                        download_url=download_url,
                        package_size_mb=package_result['zip_size_mb']
                    )
            except Exception as e:
                logger.error(f"[{task_id}] Email error: {e}")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'Funding package generated from documents successfully! ðŸŽ‰',
            'workflow': 'document_first',
            'generation': {
                'documents_generated': len([d for d in all_documents if d.get('success')]),
                'documents_refined': len([d for d in all_documents if d.get('refined')]),
                'total_pages': sum([d.get('pages', 0) for d in all_documents])
            },
            'package': {
                'filename': package_result['zip_filename'],
                'size_mb': package_result['zip_size_mb'],
                'download_url': download_url
            },
            'delivery': {
                'email_sent': email_sent,
                'email': email if email_sent else None
            },
            'timestamp': datetime.utcnow().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"Document-first workflow failed: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'Document-first workflow failed'
        }), 500


@real_funding_v2.route('/v2/funding/health', methods=['GET'])
def health_check_v2():
    """Check if all systems are operational"""
    try:
        generator = get_document_generator()
        email_service = EmailService()
        package_manager = get_package_manager()
        
        systems = {
            'ai_generation': {
                'status': 'configured' if generator.enabled else 'not_configured',
                'model': 'gemini-1.5-pro' if generator.enabled else None
            },
            'document_conversion': {
                'status': 'ready',
                'formats': ['pdf', 'word', 'pptx']
            },
            'email_delivery': {
                'status': 'configured' if email_service.is_configured() else 'not_configured'
            },
            'cloud_storage': {
                'status': 'configured' if package_manager.s3_client else 'local_only',
                'provider': 's3' if package_manager.s3_client else 'filesystem'
            }
        }
        
        all_ready = (
            generator.enabled and
            email_service.is_configured() and
            package_manager is not None
        )
        
        return jsonify({
            'success': True,
            'status': 'fully_operational' if all_ready else 'partially_configured',
            'systems': systems,
            'version': '2.0',
            'quality_standard': 'Presidential / Fortune 50',
            'capabilities': {
                'documents': 20,
                'pages': '175+',
                'formats': ['pdf', 'word', 'pptx'],
                'delivery': ['email', 'download', 'cloud']
            }
        }), 200
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
